{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtka77voyqZx",
        "outputId": "a90aab8d-f2c0-4cc2-dc2d-5dd159a781a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\\from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths\n",
        "root_dir = \"/content/drive/MyDrive/Frames_CNN\"\n",
        "output_dir = \"/content/drive/MyDrive/Frames_seqential_split\"\n",
        "labels_path = \"/content/drive/MyDrive/Infant_data.csv\"\n",
        "\n",
        "# Load labels\n",
        "labels_df = pd.read_csv(labels_path)\n",
        "\n",
        "# Collect all infants and their classes\n",
        "infant_folders = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
        "\n",
        "infant_ids = []\n",
        "classes = []\n",
        "for folder in infant_folders:\n",
        "    if \"motion\" in folder:\n",
        "        infant_num = folder.split(\"motion\")[1].split(\"_\")[0]\n",
        "        infant_id = f\"baby{infant_num}\"\n",
        "\n",
        "        label_row = labels_df[labels_df['infant_id'] == infant_id]\n",
        "        if not label_row.empty:\n",
        "            infant_ids.append(folder)\n",
        "            classes.append(label_row['class'].values[0])\n",
        "\n",
        "# Split infants ‚Üí train/test\n",
        "train_ids, test_ids, y_train, y_test = train_test_split(\n",
        "    infant_ids, classes, test_size=0.2, random_state=42, stratify=classes\n",
        ")\n",
        "\n",
        "# Create output dirs\n",
        "for split in [\"train\", \"test\"]:\n",
        "    for cls in [\"0\", \"1\"]:\n",
        "        os.makedirs(os.path.join(output_dir, split, cls), exist_ok=True)\n",
        "\n",
        "# Copy files infant-wise\n",
        "def copy_infants(infant_list, split):\n",
        "    for folder in infant_list:\n",
        "        infant_num = folder.split(\"motion\")[1].split(\"_\")[0]\n",
        "        infant_id = f\"baby{infant_num}\"\n",
        "\n",
        "        label = labels_df[labels_df['infant_id'] == infant_id]['class'].values[0]\n",
        "        src = os.path.join(root_dir, folder)\n",
        "        dst = os.path.join(output_dir, split, str(label), folder)\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "\n",
        "copy_infants(train_ids, \"train\")\n",
        "copy_infants(test_ids, \"test\")\n",
        "\n",
        "print(\"‚úÖ Dataset split by infants completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnYH4I0Tyu6S",
        "outputId": "b29d204a-3521-4861-f4f2-70946d4b7c7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset split by infants completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, seq_len=30):\n",
        "        \"\"\"\n",
        "        root_dir: path to train/0, train/1 etc.\n",
        "        transform: transformations for frames\n",
        "        seq_len: fixed number of frames per sequence\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.seq_len = seq_len\n",
        "        self.samples = []\n",
        "\n",
        "        # Go through each class (0, 1)\n",
        "        for label in os.listdir(root_dir):\n",
        "            class_dir = os.path.join(root_dir, label)\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "\n",
        "            # Each clip folder is one sample\n",
        "            for clip in os.listdir(class_dir):\n",
        "                clip_path = os.path.join(class_dir, clip)\n",
        "                if os.path.isdir(clip_path):\n",
        "                    self.samples.append((clip_path, int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip_path, label = self.samples[idx]\n",
        "\n",
        "        # Get all frames in order\n",
        "        frames = sorted(os.listdir(clip_path))\n",
        "        frames_data = []\n",
        "\n",
        "        for frame_name in frames[:self.seq_len]:  # limit to seq_len frames\n",
        "            frame_path = os.path.join(clip_path, frame_name)\n",
        "            img = cv2.imread(frame_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            frames_data.append(img)\n",
        "\n",
        "        # If less frames than seq_len ‚Üí pad\n",
        "        while len(frames_data) < self.seq_len:\n",
        "            frames_data.append(torch.zeros_like(frames_data[0]))\n",
        "\n",
        "        # Shape: (seq_len, C, H, W)\n",
        "        frames_tensor = torch.stack(frames_data)\n",
        "\n",
        "        return frames_tensor, label\n"
      ],
      "metadata": {
        "id": "f0RRWWfbzXxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "id": "WJEWMDTG1V94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/Frames_sequential_split/train\"\n",
        "test_dir  = \"/content/drive/MyDrive/Frames_sequential_split/test\"\n",
        "\n",
        "train_dataset = SequenceDataset(train_dir, transform=transform, seq_len=30)\n",
        "test_dataset  = SequenceDataset(test_dir, transform=transform, seq_len=30)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "print(f\"Train clips: {len(train_dataset)}, Test clips: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF4QQJDr1aWH",
        "outputId": "97da9774-75ff-4577-f24f-9867bd7bb324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train clips: 61, Test clips: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample, label = train_dataset[0]\n",
        "print(\"Sample shape:\", sample.shape)  # (seq_len, C, H, W)\n",
        "print(\"Label:\", label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNUwXYm81cmv",
        "outputId": "22371c49-30a3-4010-dea9-e6809ed2dad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample shape: torch.Size([30, 3, 128, 128])\n",
            "Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, cnn_embed_dim=512, lstm_hidden=128, lstm_layers=1, num_classes=1):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "\n",
        "        # Step 1: Pretrained CNN (feature extractor)\n",
        "        self.cnn = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT).features\n",
        "        self.cnn_pool = nn.AdaptiveAvgPool2d((1, 1))  # output shape = (batch, 1280, 1, 1)\n",
        "        self.cnn_fc = nn.Linear(1280, cnn_embed_dim)  # reduce to 512-dim feature\n",
        "\n",
        "        # Step 2: LSTM for temporal modeling\n",
        "        self.lstm = nn.LSTM(input_size=cnn_embed_dim,\n",
        "                            hidden_size=lstm_hidden,\n",
        "                            num_layers=lstm_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Step 3: Fully connected for classification\n",
        "        self.fc1 = nn.Linear(lstm_hidden, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()  # binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, 3, 128, 128)\n",
        "        batch_size, seq_len, C, H, W = x.size()\n",
        "\n",
        "        # Flatten batch & seq to feed frames into CNN\n",
        "        x = x.view(batch_size * seq_len, C, H, W)\n",
        "        with torch.no_grad():  # freeze CNN weights if you want\n",
        "            cnn_features = self.cnn(x)             # (batch*seq_len, 1280, H', W')\n",
        "            cnn_features = self.cnn_pool(cnn_features)  # (batch*seq_len, 1280, 1, 1)\n",
        "            cnn_features = cnn_features.view(batch_size*seq_len, -1)  # (batch*seq_len, 1280)\n",
        "            cnn_features = self.cnn_fc(cnn_features)  # (batch*seq_len, 512)\n",
        "\n",
        "        # Reshape back to sequence\n",
        "        cnn_features = cnn_features.view(batch_size, seq_len, -1)  # (batch, seq_len, 512)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, (h_n, c_n) = self.lstm(cnn_features)  # lstm_out: (batch, seq_len, hidden)\n",
        "        last_output = lstm_out[:, -1, :]                 # take last time-step output\n",
        "\n",
        "        # Classification\n",
        "        x = self.fc1(last_output)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "cPvGVpLv1fkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the model forward pass\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNN_LSTM().to(device)\n",
        "\n",
        "# Example: batch of 4 sequences\n",
        "for batch_x, batch_y in train_loader:\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device).float().unsqueeze(1)\n",
        "    out = model(batch_x)\n",
        "    print(\"Output shape:\", out.shape)  # should be (batch_size, 1)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UFV8jOu1jW_",
        "outputId": "2250a655-85f6-4c70-befa-7942de6a5fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.6M/13.6M [00:00<00:00, 91.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ===== TRAINING =====\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device).float().unsqueeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        running_corrects += (preds == y_batch).sum().item()\n",
        "        total_samples += X_batch.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc  = running_corrects / total_samples\n",
        "\n",
        "    # ===== VALIDATION =====\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_corrects = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in test_loader:\n",
        "            X_val = X_val.to(device)\n",
        "            y_val = y_val.to(device).float().unsqueeze(1)\n",
        "\n",
        "            outputs_val = model(X_val)\n",
        "            loss_val = criterion(outputs_val, y_val)\n",
        "\n",
        "            val_loss += loss_val.item() * X_val.size(0)\n",
        "            preds_val = (outputs_val > 0.5).float()\n",
        "            val_corrects += (preds_val == y_val).sum().item()\n",
        "            val_samples += X_val.size(0)\n",
        "\n",
        "    val_epoch_loss = val_loss / val_samples\n",
        "    val_epoch_acc  = val_corrects / val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2cg0iuV1nMG",
        "outputId": "97866a41-a17d-4ae9-810c-9b6c4a257d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 0.6926, Train Acc: 0.5246 | Val Loss: 0.6922, Val Acc: 0.4375\n",
            "Epoch 2/10 | Train Loss: 0.6906, Train Acc: 0.6230 | Val Loss: 0.6898, Val Acc: 0.5625\n",
            "Epoch 3/10 | Train Loss: 0.6858, Train Acc: 0.6066 | Val Loss: 0.6862, Val Acc: 0.6250\n",
            "Epoch 4/10 | Train Loss: 0.6796, Train Acc: 0.6557 | Val Loss: 0.6820, Val Acc: 0.5625\n",
            "Epoch 5/10 | Train Loss: 0.6734, Train Acc: 0.6066 | Val Loss: 0.6770, Val Acc: 0.5625\n",
            "Epoch 6/10 | Train Loss: 0.6677, Train Acc: 0.6066 | Val Loss: 0.6721, Val Acc: 0.5625\n",
            "Epoch 7/10 | Train Loss: 0.6617, Train Acc: 0.6066 | Val Loss: 0.6667, Val Acc: 0.6250\n",
            "Epoch 8/10 | Train Loss: 0.6553, Train Acc: 0.5902 | Val Loss: 0.6534, Val Acc: 0.6250\n",
            "Epoch 9/10 | Train Loss: 0.6392, Train Acc: 0.6230 | Val Loss: 0.6374, Val Acc: 0.6250\n",
            "Epoch 10/10 | Train Loss: 0.5999, Train Acc: 0.6885 | Val Loss: 0.6004, Val Acc: 0.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Full Fine-Tuned CNN + LSTM Code with Data Augmentation"
      ],
      "metadata": {
        "id": "7bSeSzrM1seH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1Ô∏è‚É£  Imports & Setup\n",
        "# ============================================================\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"‚úÖ Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£  Data Augmentation & Normalization\n",
        "# ============================================================\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£  Sequence Dataset (CNN-LSTM Ready)\n",
        "# ============================================================\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, seq_len=20):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.seq_len = seq_len\n",
        "        self.samples = []\n",
        "\n",
        "        for label in os.listdir(root_dir):\n",
        "            class_dir = os.path.join(root_dir, label)\n",
        "            if not os.path.isdir(class_dir):\n",
        "                continue\n",
        "            for clip in os.listdir(class_dir):\n",
        "                clip_path = os.path.join(class_dir, clip)\n",
        "                if os.path.isdir(clip_path):\n",
        "                    self.samples.append((clip_path, int(label)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip_path, label = self.samples[idx]\n",
        "        frames = sorted(os.listdir(clip_path))\n",
        "        frames_data = []\n",
        "\n",
        "        # Random start for better generalization\n",
        "        start_idx = random.randint(0, max(0, len(frames) - self.seq_len))\n",
        "        selected_frames = frames[start_idx : start_idx + self.seq_len]\n",
        "\n",
        "        for frame_name in selected_frames:\n",
        "            frame_path = os.path.join(clip_path, frame_name)\n",
        "            img = cv2.imread(frame_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            frames_data.append(img)\n",
        "\n",
        "        # Pad if fewer frames\n",
        "        while len(frames_data) < self.seq_len:\n",
        "            frames_data.append(torch.zeros_like(frames_data[0]))\n",
        "\n",
        "        frames_tensor = torch.stack(frames_data)  # (seq_len, C, H, W)\n",
        "        return frames_tensor, label\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£  Load Dataset\n",
        "# ============================================================\n",
        "train_dataset = SequenceDataset(\n",
        "    \"/content/drive/MyDrive/Frames_sequential_split/train\", transform=train_transforms, seq_len=20)\n",
        "test_dataset = SequenceDataset(\n",
        "    \"/content/drive/MyDrive/Frames_sequential_split/test\", transform=test_transforms, seq_len=20)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)} | Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£  CNN + LSTM Model with MobileNetV2 Backbone\n",
        "# ============================================================\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, num_layers=1, num_classes=2):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "        self.cnn = mobilenet.features\n",
        "        self.cnn_out_dim = 1280  # MobileNetV2 output channels\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=self.cnn_out_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=0.3)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):  # (B, T, C, H, W)\n",
        "        b, t, c, h, w = x.size()\n",
        "        cnn_out = []\n",
        "        for i in range(t):\n",
        "            frame_feat = self.cnn(x[:, i, :, :, :])\n",
        "            frame_feat = torch.mean(frame_feat, dim=[2, 3])  # Global Average Pool\n",
        "            cnn_out.append(frame_feat)\n",
        "        cnn_out = torch.stack(cnn_out, dim=1)  # (B, T, feat)\n",
        "        lstm_out, _ = self.lstm(cnn_out)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£  Initialize Model, Loss, Optimizer\n",
        "# ============================================================\n",
        "model = CNN_LSTM().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# ============================================================\n",
        "# 7Ô∏è‚É£  Training Loop with Checkpoint\n",
        "# ============================================================\n",
        "best_val_acc = 0.0\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # ---- Training ----\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0, 0, 0\n",
        "    for frames, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = train_correct / total\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_loader:\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}] \"\n",
        "          f\"| Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f} \"\n",
        "          f\"| Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_cnn_lstm_model.pth\")\n",
        "        print(\"‚úÖ Saved best model!\")\n",
        "\n",
        "print(f\"üèÅ Training completed. Best Val Accuracy: {best_val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bst_Ya3vBXZ2",
        "outputId": "739a134c-df3d-4a44-9f8a-20c8daebcf3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Device: cuda\n",
            "Train samples: 61 | Test samples: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:31<00:00,  1.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25] | Train Loss: 0.6546, Train Acc: 0.6066 | Val Loss: 0.4318, Val Acc: 0.8750\n",
            "‚úÖ Saved best model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:31<00:00,  1.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/25] | Train Loss: 0.4299, Train Acc: 0.8197 | Val Loss: 0.3877, Val Acc: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:36<00:00,  2.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/25] | Train Loss: 0.3266, Train Acc: 0.8689 | Val Loss: 0.3869, Val Acc: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:33<00:00,  2.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/25] | Train Loss: 0.4015, Train Acc: 0.8525 | Val Loss: 0.2261, Val Acc: 0.9375\n",
            "‚úÖ Saved best model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:30<00:00,  1.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/25] | Train Loss: 0.2677, Train Acc: 0.9180 | Val Loss: 0.2054, Val Acc: 1.0000\n",
            "‚úÖ Saved best model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:32<00:00,  2.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/25] | Train Loss: 0.2035, Train Acc: 0.9344 | Val Loss: 0.1885, Val Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:32<00:00,  2.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/25] | Train Loss: 0.2824, Train Acc: 0.9016 | Val Loss: 0.1411, Val Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:33<00:00,  2.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/25] | Train Loss: 0.1649, Train Acc: 0.9344 | Val Loss: 0.1395, Val Acc: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:31<00:00,  1.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/25] | Train Loss: 0.1837, Train Acc: 0.9344 | Val Loss: 0.2184, Val Acc: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:32<00:00,  2.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/25] | Train Loss: 0.3288, Train Acc: 0.9508 | Val Loss: 0.9545, Val Acc: 0.6250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:30<00:00,  1.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/25] | Train Loss: 0.1637, Train Acc: 0.9508 | Val Loss: 0.3542, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:32<00:00,  2.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/25] | Train Loss: 0.4355, Train Acc: 0.8525 | Val Loss: 0.8797, Val Acc: 0.7500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:30<00:00,  1.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/25] | Train Loss: 0.1410, Train Acc: 0.9344 | Val Loss: 0.1813, Val Acc: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:33<00:00,  2.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/25] | Train Loss: 0.3169, Train Acc: 0.9180 | Val Loss: 1.1032, Val Acc: 0.5625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:30<00:00,  1.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/25] | Train Loss: 0.2310, Train Acc: 0.9016 | Val Loss: 0.2377, Val Acc: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:34<00:00,  2.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/25] | Train Loss: 0.4004, Train Acc: 0.8197 | Val Loss: 0.3238, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:34<00:00,  2.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/25] | Train Loss: 0.2739, Train Acc: 0.9344 | Val Loss: 0.8540, Val Acc: 0.5625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:30<00:00,  1.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/25] | Train Loss: 0.1322, Train Acc: 0.9508 | Val Loss: 0.1819, Val Acc: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:33<00:00,  2.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/25] | Train Loss: 0.1953, Train Acc: 0.9508 | Val Loss: 0.2358, Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:33<00:00,  2.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/25] | Train Loss: 0.2041, Train Acc: 0.9508 | Val Loss: 0.8125, Val Acc: 0.5625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:34<00:00,  2.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/25] | Train Loss: 0.2862, Train Acc: 0.8852 | Val Loss: 0.6827, Val Acc: 0.6250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:31<00:00,  1.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/25] | Train Loss: 0.1110, Train Acc: 0.9836 | Val Loss: 0.1688, Val Acc: 0.9375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:31<00:00,  2.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/25] | Train Loss: 0.2268, Train Acc: 0.8689 | Val Loss: 0.0930, Val Acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:34<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/25] | Train Loss: 0.2556, Train Acc: 0.9180 | Val Loss: 0.9017, Val Acc: 0.5625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:37<00:00,  2.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/25] | Train Loss: 0.3015, Train Acc: 0.9016 | Val Loss: 0.6717, Val Acc: 0.6250\n",
            "üèÅ Training completed. Best Val Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Early Stopping"
      ],
      "metadata": {
        "id": "DspWSyvECsny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# your CNN + LSTM model here (keep as is)\n",
        "# model = CNN_LSTM(...)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "num_epochs = 25\n",
        "patience = 3  # stop if validation acc doesn‚Äôt improve for 3 consecutive epochs\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "best_model_path = \"best_cnn_lstm_model.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # ---- Early Stopping Logic ----\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"‚úÖ Saved best model at epoch {epoch+1} (Val Acc: {val_acc:.4f})\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"‚ö†Ô∏è No improvement for {epochs_no_improve} epochs.\")\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\nüõë Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nTraining completed. Best validation accuracy: {best_val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXg-MznvCswi",
        "outputId": "83c1df6e-55c3-496d-f256-4a7316b3225e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25] | Train Loss: 0.2499, Train Acc: 0.9016 | Val Loss: 0.9514, Val Acc: 0.5625\n",
            "‚úÖ Saved best model at epoch 1 (Val Acc: 0.5625)\n",
            "Epoch [2/25] | Train Loss: 0.1434, Train Acc: 0.9508 | Val Loss: 0.1281, Val Acc: 0.9375\n",
            "‚úÖ Saved best model at epoch 2 (Val Acc: 0.9375)\n",
            "Epoch [3/25] | Train Loss: 0.4749, Train Acc: 0.8033 | Val Loss: 0.8427, Val Acc: 0.6250\n",
            "‚ö†Ô∏è No improvement for 1 epochs.\n",
            "Epoch [4/25] | Train Loss: 0.1961, Train Acc: 0.9180 | Val Loss: 1.0305, Val Acc: 0.5625\n",
            "‚ö†Ô∏è No improvement for 2 epochs.\n",
            "Epoch [5/25] | Train Loss: 0.1866, Train Acc: 0.9344 | Val Loss: 0.2421, Val Acc: 0.9375\n",
            "‚ö†Ô∏è No improvement for 3 epochs.\n",
            "\n",
            "üõë Early stopping at epoch 5\n",
            "\n",
            "Training completed. Best validation accuracy: 0.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Make sure you have the same model class used during training ---\n",
        "# Example: model = CNN_LSTM(...)\n",
        "model = CNN_LSTM(num_classes=2)   # or the correct number of classes\n",
        "model.load_state_dict(torch.load(\"best_cnn_lstm_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Best model loaded for prediction.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYTaJtJeExLY",
        "outputId": "19309d3f-8757-4029-9919-d1627ac48bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Best model loaded for prediction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Use same transform as test_transforms used in training\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load frames\n",
        "def load_sequence_frames(folder_path, seq_len=20):\n",
        "    frames = sorted(os.listdir(folder_path))[:seq_len]\n",
        "    imgs = []\n",
        "    for f in frames:\n",
        "        img_path = os.path.join(folder_path, f)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = test_transforms(img)\n",
        "        imgs.append(img)\n",
        "    # Shape: (seq_len, 3, 224, 224)\n",
        "    sequence = torch.stack(imgs)\n",
        "    # Add batch dimension: (1, seq_len, 3, 224, 224)\n",
        "    return sequence.unsqueeze(0)\n",
        "\n",
        "# Example\n",
        "sequence = load_sequence_frames(\"/content/drive/MyDrive/Frames_seqential_split/test/0/skeleton_slow_resized_baby_motion10_25fps_clip1\", seq_len=20).to(device)\n"
      ],
      "metadata": {
        "id": "qp1z6Z1-ExPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(sequence)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(f\"Predicted class: {predicted.item()}\")\n"
      ],
      "metadata": {
        "id": "pQctt1dyExSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d479040-ff84-4a78-d95a-0825b615233e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"Normal\", 1: \"Abnormal\"}\n",
        "print(\"Predicted:\", label_map[predicted.item()])\n"
      ],
      "metadata": {
        "id": "rKhUCu8yExVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a120e880-e905-4bf9-d73b-3bd3831ad792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Normal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Use same transform as test_transforms used in training\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load frames\n",
        "def load_sequence_frames(folder_path, seq_len=20):\n",
        "    frames = sorted(os.listdir(folder_path))[:seq_len]\n",
        "    imgs = []\n",
        "    for f in frames:\n",
        "        img_path = os.path.join(folder_path, f)\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = test_transforms(img)\n",
        "        imgs.append(img)\n",
        "    # Shape: (seq_len, 3, 224, 224)\n",
        "    sequence = torch.stack(imgs)\n",
        "    # Add batch dimension: (1, seq_len, 3, 224, 224)\n",
        "    return sequence.unsqueeze(0)\n",
        "\n",
        "# Example\n",
        "sequence = load_sequence_frames(\"/content/drive/MyDrive/Frames_seqential_split/test/1/skeleton_slow_resized_baby_motion8_25fps_clip1\", seq_len=20).to(device)\n"
      ],
      "metadata": {
        "id": "kC4lBCSHExYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(sequence)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    print(f\"Predicted class: {predicted.item()}\")\n"
      ],
      "metadata": {
        "id": "hDE-Pa9-ExbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cd350b-5ab3-47d5-9fe3-f4f6177fe6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {0: \"Normal\", 1: \"Abnormal\"}\n",
        "print(\"Predicted:\", label_map[predicted.item()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7pCS9fvFnQF",
        "outputId": "6bcc8a5c-7df3-458c-8a90-ee38d6d4d5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Abnormal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nj8MQqXiK2Us"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}